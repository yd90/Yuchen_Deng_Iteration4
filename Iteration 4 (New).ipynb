{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BDAS').getOrCreate()\n",
    "\n",
    "# Importing data which has a header. Schema is automatically configured.\n",
    "df = spark.read.csv('Dataset.csv', header=True, inferSchema=True)\n",
    "\n",
    "#Show the data table\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('No', 'Year', 'Month','day','Hour').describe().show()\n",
    "df.select('PM2-5','PM10','SO2','NO2','CO','O3').describe().show()\n",
    "df.select('TEMP','PRES','DEWP','RAIN','WD','WSPM','Station').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Show the relationship between PM10 and PM2.5\n",
    "x=df.toPandas()['PM10']\n",
    "y=df.toPandas()['PM2-5']\n",
    "plt.xlabel('PM10')\n",
    "plt.ylabel('PM2-5')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the relationship between SO2 and PM2.5\n",
    "x=df.toPandas()['SO2']\n",
    "y=df.toPandas()['PM2-5']\n",
    "plt.xlabel('SO2')\n",
    "plt.ylabel('PM2-5')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the relationship between NO2 and PM2.5\n",
    "x=df.toPandas()['NO2']\n",
    "y=df.toPandas()['PM2-5']\n",
    "plt.xlabel('NO2')\n",
    "plt.ylabel('PM2-5')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the relationship between CO and PM2.5\n",
    "x=df.toPandas()['CO']\n",
    "y=df.toPandas()['PM2-5']\n",
    "plt.xlabel('CO')\n",
    "plt.ylabel('PM2-5')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the relationship between O3 and PM2.5\n",
    "x=df.toPandas()['O3']\n",
    "y=df.toPandas()['PM2-5']\n",
    "plt.xlabel('O3')\n",
    "plt.ylabel('PM2-5')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the relationship between TEMP and PM2.5\n",
    "x=df.toPandas()['TEMP']\n",
    "y=df.toPandas()['PM2-5']\n",
    "plt.xlabel('TEMP')\n",
    "plt.ylabel('PM2-5')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the relationship between WSPM and PM2.5\n",
    "x=df.toPandas()['WSPM']\n",
    "y=df.toPandas()['PM2-5']\n",
    "plt.xlabel('WSPM')\n",
    "plt.ylabel('PM2-5')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the null value for each column\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df.drop('No','WD','Station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any row with missing data\n",
    "df=df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the null value for each column\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('PM2-5').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('PM10').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('SO2').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('NO2').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('CO').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('O3').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('RAIN').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('WSPM').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert pyspark dataframe to pandas dataframe\n",
    "#In order to add new attribute\n",
    "df=df.toPandas()\n",
    "\n",
    "#Adding a new attribute\n",
    "def get_grade(value):\n",
    "    if value <= 12 and value>=0:\n",
    "        return 'Good'\n",
    "    elif value <= 35:\n",
    "        return 'Moderate'\n",
    "    elif value <= 55:\n",
    "        return 'Unhealthy for Sensi'\n",
    "    elif value <= 150:\n",
    "        return 'Unhealthy'\n",
    "    elif value <= 250:\n",
    "        return 'Very Unhealthy'\n",
    "    elif value <= 500:\n",
    "        return 'Hazardous'\n",
    "    elif value > 500:\n",
    "        return 'Beyond Index'\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "#Adding new attribute to the original table\n",
    "df.loc[:, \"PM2-5_Grade\"] = df[\"PM2-5\"].apply(get_grade)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Convert pandas dataframe to pyspark dataframe\n",
    "#In order to add new attribute\n",
    "df=spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change data type\n",
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn(\"Year\", df[\"Year\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Month\", df[\"Year\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"day\", df[\"Year\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Hour\", df[\"Year\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change data type\n",
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"CO\", df[\"CO\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Year: \", df.corr(\"PM2-5\", \"Year\"))\n",
    "print(\"Month: \", df.corr(\"PM2-5\", \"Month\"))\n",
    "print(\"day: \", df.corr(\"PM2-5\", \"day\"))\n",
    "print(\"Hour: \", df.corr(\"PM2-5\", \"Hour\"))\n",
    "print(\"PM10: \", df.corr(\"PM2-5\", \"PM10\"))\n",
    "print(\"SO2: \", df.corr(\"PM2-5\", \"SO2\"))\n",
    "print(\"NO2: \", df.corr(\"PM2-5\", \"NO2\"))\n",
    "print(\"CO: \", df.corr(\"PM2-5\", \"CO\"))\n",
    "print(\"O3: \", df.corr(\"PM2-5\", \"O3\"))\n",
    "print(\"TEMP: \", df.corr(\"PM2-5\", \"TEMP\"))\n",
    "print(\"PRES: \", df.corr(\"PM2-5\", \"PRES\"))\n",
    "print(\"DEWP: \", df.corr(\"PM2-5\", \"DEWP\"))\n",
    "print(\"RAIN: \", df.corr(\"PM2-5\", \"RAIN\"))\n",
    "print(\"WSPM: \", df.corr(\"PM2-5\", \"WSPM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop attributes\n",
    "df=df.drop('Year','Month','day','Hour','PRES','RAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = spark.read.csv('1.csv', header=True, inferSchema=True)\n",
    "file2 = spark.read.csv('2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "unionDF = file1.union(file2)\n",
    "unionDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unionDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Using log function to achieve data transformation\n",
    "#Convert pyspark dataframe to pandas dataframe\n",
    "df=df.toPandas()\n",
    "df['PM2-5_log'] = np.log(df['PM2-5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PM10_log'] = np.log(df['PM10'])\n",
    "df['SO2_log'] = np.log(df['SO2'])\n",
    "df['NO2_log'] = np.log(df['NO2'])\n",
    "df['CO_log'] = np.log(df['CO'])\n",
    "df['O3_log'] = np.log(df['O3'])\n",
    "\n",
    "#Drop the value equal to 0, because log(0) will pop up error message\n",
    "df = df.drop(df[df['WSPM'] == 0].index)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df['WSPM_log'] = np.log(df['WSPM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert pandas dataframe to pyspark dataframe\n",
    "#In order to add new attribute\n",
    "df=spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop attributes\n",
    "df=df.drop('PM2-5','PM10','SO2','NO2','CO','O3', 'WSPM')\n",
    "df=df.drop('PM2-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"TEMP\",\"TEMP_Transf\")\n",
    "df = df.withColumnRenamed(\"DEWP\",\"DEWP_Transf\")\n",
    "df = df.withColumnRenamed(\"PM2-5_log\",\"PM2-5_Transf\")\n",
    "df = df.withColumnRenamed(\"PM10_log\",\"PM10_Transf\")\n",
    "df = df.withColumnRenamed(\"SO2_log\",\"SO2_Transf\")\n",
    "df = df.withColumnRenamed(\"NO2_log\",\"NO2_Transf\")\n",
    "df = df.withColumnRenamed(\"CO_log\",\"CO_Transf\")\n",
    "df = df.withColumnRenamed(\"O3_log\",\"O3_Transf\")\n",
    "df = df.withColumnRenamed(\"WSPM_log\",\"WSPM_Transf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('PM2-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['TEMP_Transf', 'DEWP_Transf', 'PM10_Transf', 'SO2_Transf', 'NO2_Transf', 'CO_Transf', 'O3_Transf', 'WSPM_Transf'], outputCol = 'features')\n",
    "vector_output = vectorAssembler.transform(df)\n",
    "vector_output.printSchema()\n",
    "vector_output.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_output = vector_output.select(['features', 'PM2-5_Transf'])\n",
    "print(vector_output.head(1))\n",
    "vector_output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the split between training/test as a list.\n",
    "# This is based on your test-designs,70/30 splits is used. \n",
    "train_data,test_data = vector_output.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out our training data.\n",
    "train_data.show()\n",
    "test_data.show()\n",
    "# Let's check out the count.\n",
    "train_data.describe().show()\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "#Linear Regression Model\n",
    "lr1 = LinearRegression(featuresCol='features', labelCol='PM2-5_Transf', maxIter=0, regParam=0)\n",
    "\n",
    "# Fit the training data.\n",
    "lr_model = lr1.fit(train_data)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intercept\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "# Summarise the model\n",
    "training_summary = lr_model.summary\n",
    "\n",
    "#R2\n",
    "print(\"R2: \" + str(training_summary.r2))\n",
    "\n",
    "#RMSE\n",
    "print(\"RMSE: \" + str(training_summary.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the feature labels\n",
    "feat_labels = ['TEMP_Transf', 'DEWP_Transf','PM10_Transf','SO2_Transf','NO2_Transf','CO_Transf','O3_Transf','WSPM_Transf']\n",
    "\n",
    "#Show the coefficients \n",
    "coefficients =lr_model.coefficients \n",
    "indices = np.argsort(coefficients)[::-1]\n",
    "for f in range(train_data.toPandas().shape[1]):\n",
    "    print(\"%2d: %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], coefficients[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "#Importance Graph\n",
    "x_columns = np.array(['PM10','CO','NO2','O3','SO2','DEWP','WSPM','TEMP'])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Coefficients\",fontsize = 18)\n",
    "plt.ylabel(\"Coefficients level\",fontsize = 14)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.ylim(0, 1)\n",
    "y_major_locator=MultipleLocator(0.1)\n",
    "ax=plt.gca()\n",
    "ax.yaxis.set_major_locator(y_major_locator)\n",
    "\n",
    "#Get every coefficient\n",
    "coefficients =lr_model.coefficients \n",
    "for i in range(x_columns.shape[0]):\n",
    "    plt.bar(i,coefficients[indices[i]],color='orange',align='center')\n",
    "    plt.xticks(np.arange(x_columns.shape[0]),x_columns,fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "#Linear Regression Model\n",
    "lr2 = LinearRegression(featuresCol='features', labelCol='PM2-5_Transf', maxIter=10, regParam=0)\n",
    "\n",
    "# Fit the training data.\n",
    "lr_model = lr2.fit(train_data)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "#Importance Graph\n",
    "x_columns = np.array(['PM10','CO','NO2','O3','SO2','DEWP','WSPM','TEMP'])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Coefficients\",fontsize = 18)\n",
    "plt.ylabel(\"Coefficients level\",fontsize = 14)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.ylim(0, 1)\n",
    "y_major_locator=MultipleLocator(0.1)\n",
    "ax=plt.gca()\n",
    "ax.yaxis.set_major_locator(y_major_locator)\n",
    "\n",
    "#Get every coefficient\n",
    "coefficients =lr_model.coefficients \n",
    "for i in range(x_columns.shape[0]):\n",
    "    plt.bar(i,coefficients[indices[i]],color='orange',align='center')\n",
    "    plt.xticks(np.arange(x_columns.shape[0]),x_columns,fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "#Linear Regression Model\n",
    "lr3= LinearRegression(featuresCol='features', labelCol='PM2-5_Transf', maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the training data.\n",
    "lr_model = lr3.fit(train_data)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "#Importance Graph\n",
    "x_columns = np.array(['PM10','CO','NO2','O3','SO2','DEWP','WSPM','TEMP'])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Coefficients\",fontsize = 18)\n",
    "plt.ylabel(\"Coefficients level\",fontsize = 14)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.ylim(0, 1)\n",
    "y_major_locator=MultipleLocator(0.1)\n",
    "ax=plt.gca()\n",
    "ax.yaxis.set_major_locator(y_major_locator)\n",
    "\n",
    "#Get every coefficient\n",
    "coefficients =lr_model.coefficients \n",
    "for i in range(x_columns.shape[0]):\n",
    "    plt.bar(i,coefficients[indices[i]],color='orange',align='center')\n",
    "    plt.xticks(np.arange(x_columns.shape[0]),x_columns,fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package \n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "random_forest1 = RandomForestRegressor(numTrees=5,featuresCol='features', labelCol='PM2-5_Transf')\n",
    "rf1 = random_forest1.fit(train_data)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(\"Coefficients: \" + str(rf1.featureImportances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package \n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "random_forest2 = RandomForestRegressor(numTrees=10,featuresCol='features', labelCol='PM2-5_Transf')\n",
    "rf2 = random_forest2.fit(train_data)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(\"Coefficients: \" + str(rf2.featureImportances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package \n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "random_forest3 = RandomForestRegressor(numTrees=100,featuresCol='features', labelCol='PM2-5_Transf')\n",
    "rf3 = random_forest3.fit(train_data)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(\"Coefficients: \" + str(rf3.featureImportances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"PM2-5_Transf\", predictionCol=\"prediction\", metricName='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear\n",
    "lr1= LinearRegression(featuresCol='features', labelCol='PM2-5_Transf', maxIter=0, regParam=0)\n",
    "lr2= LinearRegression(featuresCol='features', labelCol='PM2-5_Transf', maxIter=10, regParam=0)\n",
    "lr3= LinearRegression(featuresCol='features', labelCol='PM2-5_Transf', maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the training data\n",
    "lr_model1 = lr1.fit(train_data)\n",
    "lr_model2 = lr2.fit(train_data)\n",
    "lr_model3 = lr3.fit(train_data)\n",
    "\n",
    "#Prediction\n",
    "pred1 = lr_model1.transform(test_data)\n",
    "pred2 = lr_model2.transform(test_data)\n",
    "pred3 = lr_model3.transform(test_data)\n",
    "\n",
    "#Evaluation\n",
    "e1 = evaluator.evaluate(pred1)\n",
    "e2 = evaluator.evaluate(pred1)\n",
    "e3 = evaluator.evaluate(pred1)\n",
    "\n",
    "#Output\n",
    "print(\"R2 Model 1: \" + str(e1))\n",
    "print(\"R2 Model 2: \" + str(e2))\n",
    "print(\"R2 Model 3: \" + str(e3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest\n",
    "random_forest1 = RandomForestRegressor(numTrees=5,featuresCol='features', labelCol='PM2-5_Transf')\n",
    "random_forest2 = RandomForestRegressor(numTrees=10,featuresCol='features', labelCol='PM2-5_Transf')\n",
    "random_forest3 = RandomForestRegressor(numTrees=100,featuresCol='features', labelCol='PM2-5_Transf')\n",
    "\n",
    "rf1 = random_forest1.fit(train_data)\n",
    "rf2 = random_forest2.fit(train_data)\n",
    "rf3 = random_forest3.fit(train_data)\n",
    "\n",
    "#Prediction\n",
    "pred1 = rf1.transform(test_data)\n",
    "pred2 = rf2.transform(test_data)\n",
    "pred3 = rf3.transform(test_data)\n",
    "\n",
    "#Evaluation\n",
    "e1 = evaluator.evaluate(pred1)\n",
    "e2 = evaluator.evaluate(pred2)\n",
    "e3 = evaluator.evaluate(pred3)\n",
    "\n",
    "#Output\n",
    "print(\"Random Forest Regression Model 1: \" + str(e1))\n",
    "print(\"Random Forest Regression Model 2: \" + str(e2))\n",
    "print(\"Random Forest Regression Model 3: \" + str(e3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest3 = RandomForestRegressor(numTrees=100,featuresCol='features', labelCol='PM2-5_Transf')\n",
    "rf3 = random_forest3.fit(train_data)\n",
    "pred3 = rf3.transform(test_data)\n",
    "pred3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
